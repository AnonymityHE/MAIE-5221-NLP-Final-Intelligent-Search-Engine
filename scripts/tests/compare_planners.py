"""
å¯¹æ¯”HKGAI vs Geminiä½œä¸ºå·¥ä½œæµè§„åˆ’å™¨çš„æ•ˆæœ
"""
import sys
import os
from typing import Dict, List
import json

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from services.agent.workflow_llm_planner import WorkflowPlan, WorkflowStep
from services.llm.unified_client import unified_llm_client
from services.core.config import settings
from services.core.logger import logger


# æ‰©å±•çš„æµ‹è¯•é—®é¢˜é›†
COMPREHENSIVE_TESTS = [
    # ç®€å•æŸ¥è¯¢
    {
        "id": 1,
        "query": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "expected_workflow": False,
        "category": "çŸ¥è¯†é—®ç­”"
    },
    {
        "id": 2,
        "query": "è‹¹æœå…¬å¸çš„è‚¡ç¥¨ä»£ç æ˜¯ä»€ä¹ˆï¼Ÿ",
        "expected_workflow": False,
        "category": "ç®€å•äº‹å®æŸ¥è¯¢"
    },
    # é‡‘èå¯¹æ¯”
    {
        "id": 3,
        "query": "Compare the stock performance of Tesla and BYD in the last month",
        "expected_workflow": True,
        "category": "é‡‘èå¯¹æ¯”ï¼ˆè‹±æ–‡ï¼‰"
    },
    {
        "id": 4,
        "query": "åˆ†æå¾®è½¯ã€è‹¹æœå’Œè°·æ­Œä¸‰å®¶å…¬å¸çš„è‚¡ä»·å¯¹æ¯”",
        "expected_workflow": True,
        "category": "å¤šç›®æ ‡é‡‘èå¯¹æ¯”ï¼ˆä¸­æ–‡ï¼‰"
    },
    # è·¨é¢†åŸŸ
    {
        "id": 5,
        "query": "é¦™æ¸¯ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Œé€‚åˆå»æ·±åœ³æ—…æ¸¸å—ï¼Ÿäº¤é€šéœ€è¦å¤šä¹…ï¼Ÿ",
        "expected_workflow": True,
        "category": "è·¨é¢†åŸŸç»¼åˆ"
    },
    # å¤æ‚åˆ†æ
    {
        "id": 6,
        "query": "What was the impact of the latest NVIDIA earnings report on their stock price?",
        "expected_workflow": True,
        "category": "é¡¹ç›®å…¬å‘Šç¤ºä¾‹"
    },
    # æ—¶åºå¯¹æ¯”
    {
        "id": 7,
        "query": "å¯¹æ¯”ä¸€ä¸‹æ¯”ç‰¹å¸å’Œä»¥å¤ªåŠæœ€è¿‘ä¸€å‘¨çš„ä»·æ ¼èµ°åŠ¿",
        "expected_workflow": True,
        "category": "åŠ å¯†è´§å¸å¯¹æ¯”"
    },
    # è¾¹ç•Œæ¡ˆä¾‹
    {
        "id": 8,
        "query": "Tell me about recent AI developments",
        "expected_workflow": False,  # å¯èƒ½è§¦å‘ä¹Ÿå¯èƒ½ä¸è§¦å‘
        "category": "å®æ—¶ä¿¡æ¯"
    },
]


class GeminiPlanner:
    """ä½¿ç”¨Gemini Flashä½œä¸ºè§„åˆ’å™¨"""
    
    def __init__(self, tools: List[str]):
        self.available_tools = tools
        logger.info("åˆå§‹åŒ–Geminiè§„åˆ’å™¨ï¼ˆä½¿ç”¨gemini-flash-expï¼‰")
    
    def analyze_query(self, query: str) -> WorkflowPlan:
        """ä½¿ç”¨Geminiåˆ†ææŸ¥è¯¢"""
        system_prompt = self._build_planner_prompt()
        user_prompt = f"""
è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦å¤šæ­¥éª¤å·¥ä½œæµï¼Œå¹¶ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚
"""
        
        try:
            # ä½¿ç”¨Gemini Flash
            llm_result = unified_llm_client.chat(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=1500,
                temperature=0.3,
                model="gemini-2.0-flash-exp",  # ä½¿ç”¨flashç‰ˆæœ¬
                provider="gemini"
            )
            
            if "error" in llm_result:
                logger.error(f"Geminiè§„åˆ’å¤±è´¥: {llm_result['error']}")
                return self._create_simple_plan(query)
            
            # è§£æJSON
            plan_json = self._extract_json(llm_result.get("content", ""))
            if not plan_json:
                return self._create_simple_plan(query)
            
            return self._parse_plan_json(plan_json, query)
            
        except Exception as e:
            logger.error(f"Geminiå·¥ä½œæµè§„åˆ’å¼‚å¸¸: {e}")
            return self._create_simple_plan(query)
    
    def _build_planner_prompt(self) -> str:
        """æ„å»ºGeminiè§„åˆ’å™¨çš„ç³»ç»Ÿæç¤ºè¯"""
        tools_desc = "\n".join([f"- {t}" for t in self.available_tools])
        
        return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å·¥ä½œæµè§„åˆ’å™¨ã€‚

å¯ç”¨å·¥å…·ï¼š
{tools_desc}

åˆ¤æ–­æ ‡å‡† - éœ€è¦å·¥ä½œæµçš„æƒ…å†µï¼š
- æ¶‰åŠå¯¹æ¯”åˆ†æï¼ˆå¦‚"æ¯”è¾ƒAå’ŒB"ï¼‰
- éœ€è¦å¤šä¸ªæ•°æ®æº
- åŒ…å«å¤šä¸ªå­é—®é¢˜
- éœ€è¦æ—¶åºåˆ†æ

åˆ¤æ–­æ ‡å‡† - ä¸éœ€è¦å·¥ä½œæµçš„æƒ…å†µï¼š
- ç®€å•çš„å•ä¸€æŸ¥è¯¢
- åªéœ€è¦ä¸€ä¸ªå·¥å…·
- çŸ¥è¯†é—®ç­”ç±»å‹

è¿”å›JSONæ ¼å¼ï¼ˆåªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "requires_workflow": true/false,
    "workflow_type": "å·¥ä½œæµç±»å‹",
    "reasoning": "æ¨ç†è¿‡ç¨‹",
    "confidence": 0.0-1.0,
    "steps": [
        {{
            "step_id": 1,
            "tool": "å·¥å…·åç§°",
            "action": "åŠ¨ä½œæè¿°",
            "query": "å…·ä½“æŸ¥è¯¢",
            "entities": {{}},
            "reason": "åŸå› "
        }}
    ],
    "entities": {{}}
}}
"""
    
    def _extract_json(self, response: str):
        """ä»å“åº”ä¸­æå–JSON"""
        import re
        try:
            return json.loads(response)
        except:
            json_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
            matches = re.findall(json_pattern, response, re.DOTALL)
            if matches:
                try:
                    return json.loads(matches[0])
                except:
                    pass
            
            start = response.find('{')
            end = response.rfind('}')
            if start != -1 and end != -1:
                try:
                    return json.loads(response[start:end+1])
                except:
                    pass
        return None
    
    def _parse_plan_json(self, plan_json: Dict, query: str) -> WorkflowPlan:
        """è§£æJSONä¸ºWorkflowPlan"""
        from services.agent.workflow_llm_planner import WorkflowPlan, WorkflowStep
        
        steps = []
        for s in plan_json.get("steps", []):
            step = WorkflowStep(
                step_id=s.get("step_id", len(steps) + 1),
                tool=s.get("tool", "local_rag"),
                action=s.get("action", ""),
                query=s.get("query", query),
                entities=s.get("entities", {}),
                reason=s.get("reason", "")
            )
            steps.append(step)
        
        return WorkflowPlan(
            workflow_type=plan_json.get("workflow_type", "simple"),
            requires_workflow=plan_json.get("requires_workflow", False),
            steps=steps,
            entities=plan_json.get("entities", {}),
            confidence=plan_json.get("confidence", 0.5),
            reasoning=plan_json.get("reasoning", "")
        )
    
    def _create_simple_plan(self, query: str) -> WorkflowPlan:
        """åˆ›å»ºç®€å•è®¡åˆ’"""
        from services.agent.workflow_llm_planner import WorkflowPlan, WorkflowStep
        
        return WorkflowPlan(
            workflow_type="simple_query",
            requires_workflow=False,
            steps=[],
            entities={},
            confidence=0.3,
            reasoning="Geminiè§„åˆ’å¤±è´¥ï¼Œfallback"
        )


def compare_planners(query_info: Dict, hkgai_plan: WorkflowPlan, gemini_plan: WorkflowPlan):
    """å¯¹æ¯”ä¸¤ä¸ªè§„åˆ’å™¨çš„ç»“æœ"""
    print("\n" + "="*120)
    print(f"ğŸ“ æµ‹è¯• #{query_info['id']}: {query_info['category']}")
    print("="*120)
    print(f"ğŸ” æŸ¥è¯¢: {query_info['query']}")
    print(f"ğŸ’­ é¢„æœŸ: {'éœ€è¦å·¥ä½œæµ' if query_info['expected_workflow'] else 'ä¸éœ€è¦å·¥ä½œæµ'}")
    print("\n" + "-"*120)
    
    # HKGAIç»“æœ
    print("ğŸŸ¦ HKGAIè§„åˆ’:")
    print(f"   éœ€è¦å·¥ä½œæµ: {hkgai_plan.requires_workflow}")
    print(f"   å·¥ä½œæµç±»å‹: {hkgai_plan.workflow_type}")
    print(f"   ç½®ä¿¡åº¦: {hkgai_plan.confidence:.2f}")
    print(f"   æ­¥éª¤æ•°: {len(hkgai_plan.steps)}")
    print(f"   æ¨ç†: {hkgai_plan.reasoning[:100]}...")
    
    # Geminiç»“æœ
    print("\nğŸŸ© Gemini Flashè§„åˆ’:")
    print(f"   éœ€è¦å·¥ä½œæµ: {gemini_plan.requires_workflow}")
    print(f"   å·¥ä½œæµç±»å‹: {gemini_plan.workflow_type}")
    print(f"   ç½®ä¿¡åº¦: {gemini_plan.confidence:.2f}")
    print(f"   æ­¥éª¤æ•°: {len(gemini_plan.steps)}")
    print(f"   æ¨ç†: {gemini_plan.reasoning[:100]}...")
    
    # å¯¹æ¯”åˆ†æ
    print("\n" + "-"*120)
    print("ğŸ“Š å¯¹æ¯”åˆ†æ:")
    
    # åˆ¤æ–­å‡†ç¡®æ€§
    hkgai_correct = hkgai_plan.requires_workflow == query_info['expected_workflow']
    gemini_correct = gemini_plan.requires_workflow == query_info['expected_workflow']
    
    if hkgai_correct and gemini_correct:
        print("  âœ… ä¸¤è€…åˆ¤æ–­éƒ½æ­£ç¡®")
    elif hkgai_correct:
        print("  ğŸŸ¦ HKGAIåˆ¤æ–­æ­£ç¡®ï¼ŒGeminiåˆ¤æ–­é”™è¯¯")
    elif gemini_correct:
        print("  ğŸŸ© Geminiåˆ¤æ–­æ­£ç¡®ï¼ŒHKGAIåˆ¤æ–­é”™è¯¯")
    else:
        print("  âŒ ä¸¤è€…åˆ¤æ–­éƒ½é”™è¯¯")
    
    # å¯¹æ¯”ç½®ä¿¡åº¦
    if abs(hkgai_plan.confidence - gemini_plan.confidence) < 0.1:
        print(f"  âš–ï¸  ç½®ä¿¡åº¦ç›¸è¿‘ (å·®è·: {abs(hkgai_plan.confidence - gemini_plan.confidence):.2f})")
    elif hkgai_plan.confidence > gemini_plan.confidence:
        print(f"  ğŸŸ¦ HKGAIç½®ä¿¡åº¦æ›´é«˜ (+{hkgai_plan.confidence - gemini_plan.confidence:.2f})")
    else:
        print(f"  ğŸŸ© Geminiç½®ä¿¡åº¦æ›´é«˜ (+{gemini_plan.confidence - hkgai_plan.confidence:.2f})")
    
    # å¯¹æ¯”æ­¥éª¤æ•°
    if hkgai_plan.requires_workflow and gemini_plan.requires_workflow:
        if len(hkgai_plan.steps) == len(gemini_plan.steps):
            print(f"  âš–ï¸  æ­¥éª¤æ•°ç›¸åŒ ({len(hkgai_plan.steps)}æ­¥)")
        elif len(hkgai_plan.steps) > len(gemini_plan.steps):
            print(f"  ğŸŸ¦ HKGAIè§„åˆ’æ›´è¯¦ç»† ({len(hkgai_plan.steps)} vs {len(gemini_plan.steps)}æ­¥)")
        else:
            print(f"  ğŸŸ© Geminiè§„åˆ’æ›´è¯¦ç»† ({len(gemini_plan.steps)} vs {len(hkgai_plan.steps)}æ­¥)")
    
    print("="*120)
    
    return hkgai_correct, gemini_correct


def run_comparison():
    """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
    logger.info("\n\n" + "ğŸ”¬ HKGAI vs Geminiè§„åˆ’å™¨å¯¹æ¯”æµ‹è¯•".center(120, "="))
    
    tools = ["local_rag", "web_search", "weather", "finance", "transport"]
    
    # åˆå§‹åŒ–è§„åˆ’å™¨
    from services.agent.workflow_llm_planner import get_llm_workflow_planner
    hkgai_planner = get_llm_workflow_planner(tools)
    gemini_planner = GeminiPlanner(tools)
    
    logger.info(f"ğŸ“ æµ‹è¯•é…ç½®:")
    logger.info(f"   - HKGAIæ¨¡å‹: {settings.HKGAI_MODEL_ID}")
    logger.info(f"   - Geminiæ¨¡å‹: gemini-2.0-flash-exp")
    logger.info(f"   - æµ‹è¯•é—®é¢˜æ•°: {len(COMPREHENSIVE_TESTS)}\n")
    
    hkgai_correct = 0
    gemini_correct = 0
    
    for query_info in COMPREHENSIVE_TESTS:
        try:
            logger.info(f"\n{'='*50} æµ‹è¯• #{query_info['id']} {'='*50}")
            
            # HKGAIè§„åˆ’
            hkgai_plan = hkgai_planner.analyze_query(query_info['query'])
            
            # Geminiè§„åˆ’
            gemini_plan = gemini_planner.analyze_query(query_info['query'])
            
            # å¯¹æ¯”ç»“æœ
            h_correct, g_correct = compare_planners(query_info, hkgai_plan, gemini_plan)
            
            if h_correct:
                hkgai_correct += 1
            if g_correct:
                gemini_correct += 1
                
        except Exception as e:
            logger.error(f"æµ‹è¯• #{query_info['id']} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # æ€»ç»“
    print("\n\n" + "="*120)
    print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
    print("="*120)
    print(f"æ€»æµ‹è¯•æ•°: {len(COMPREHENSIVE_TESTS)}")
    print(f"\nğŸŸ¦ HKGAIå‡†ç¡®ç‡: {hkgai_correct}/{len(COMPREHENSIVE_TESTS)} ({hkgai_correct/len(COMPREHENSIVE_TESTS)*100:.1f}%)")
    print(f"ğŸŸ© Geminiå‡†ç¡®ç‡: {gemini_correct}/{len(COMPREHENSIVE_TESTS)} ({gemini_correct/len(COMPREHENSIVE_TESTS)*100:.1f}%)")
    
    if hkgai_correct > gemini_correct:
        print(f"\nğŸ† èƒœè€…: HKGAI (+{hkgai_correct - gemini_correct})")
    elif gemini_correct > hkgai_correct:
        print(f"\nğŸ† èƒœè€…: Gemini Flash (+{gemini_correct - hkgai_correct})")
    else:
        print("\nğŸ¤ å¹³å±€")
    
    print("\nğŸ’¡ å»ºè®®:")
    if hkgai_correct >= gemini_correct:
        print("  - HKGAIä½œä¸ºä¸»è§„åˆ’å™¨è¡¨ç°ä¼˜ç§€ï¼Œå¯ä»¥ç»§ç»­ä½¿ç”¨")
        print("  - Gemini Flashå¯ä»¥ä½œä¸ºfallbacké€‰é¡¹")
    else:
        print("  - è€ƒè™‘å°†Gemini Flashä½œä¸ºä¸»è§„åˆ’å™¨")
        print("  - HKGAIå¯ä»¥ä½œä¸ºfallbacké€‰é¡¹")
    
    print("="*120 + "\n")


if __name__ == "__main__":
    run_comparison()

