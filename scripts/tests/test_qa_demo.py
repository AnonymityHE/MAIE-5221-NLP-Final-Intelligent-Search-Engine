"""
çŸ¥è¯†åº“Q&Aæ¼”ç¤ºæµ‹è¯•
å±•ç¤ºåŸºäºæ–°æ„å»ºçš„çŸ¥è¯†åº“çš„å®é™…é—®ç­”æ•ˆæœ
"""
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

from services.agent.agent import agent
from services.core.logger import logger
import time

# è®¾è®¡å¤šæ ·åŒ–çš„æµ‹è¯•é—®é¢˜
TEST_QUESTIONS = [
    {
        "category": "ç³»ç»Ÿä½¿ç”¨",
        "language": "zh",
        "question": "å¦‚ä½•ä½¿ç”¨ç²¤è¯­è¿›è¡Œè¯­éŸ³è¾“å…¥ï¼Ÿ",
        "expected_topics": ["Whisper", "ç²¤è¯­API", "STT", "è‡ªåŠ¨æ£€æµ‹"]
    },
    {
        "category": "æŠ€æœ¯æ¶æ„",
        "language": "zh",
        "question": "ç³»ç»Ÿé‡‡ç”¨äº†ä»€ä¹ˆæ ·çš„å·¥ä½œæµæ¶æ„ï¼ŸLLMåœ¨å…¶ä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ",
        "expected_topics": ["LLMé©±åŠ¨", "è§„åˆ™å¼•æ“", "åŠ¨æ€æ‰§è¡Œ", "å·¥å…·è°ƒç”¨"]
    },
    {
        "category": "åŠŸèƒ½æŸ¥è¯¢",
        "language": "en",
        "question": "What tools are available in this system and what can they do?",
        "expected_topics": ["local_rag", "web_search", "finance", "weather", "transport"]
    },
    {
        "category": "æŠ€æœ¯ç»†èŠ‚",
        "language": "en",
        "question": "What is reranking and why is it important in RAG systems?",
        "expected_topics": ["cross-encoder", "relevance", "semantic", "ranking"]
    },
    {
        "category": "å¤šè¯­è¨€æ”¯æŒ",
        "language": "zh",
        "question": "è¿™ä¸ªç³»ç»Ÿæ”¯æŒå“ªäº›è¯­è¨€ï¼Ÿåˆ†åˆ«åœ¨å“ªäº›æ–¹é¢æ”¯æŒï¼Ÿ",
        "expected_topics": ["ç²¤è¯­", "æ™®é€šè¯", "è‹±è¯­", "STT", "TTS", "embedding"]
    },
    {
        "category": "APIå¯¹æ¯”",
        "language": "en",
        "question": "What's the difference between HKGAI and Gemini APIs? When does the system use each one?",
        "expected_topics": ["primary", "fallback", "quota", "failure", "automatic"]
    },
    {
        "category": "çŸ¥è¯†åº“ç®¡ç†",
        "language": "zh",
        "question": "æˆ‘æƒ³æ·»åŠ æ–°æ–‡æ¡£åˆ°çŸ¥è¯†åº“ï¼Œæœ‰ä»€ä¹ˆæ–¹æ³•ï¼Ÿ",
        "expected_topics": ["ä¸Šä¼ ", "API", "å‘½ä»¤è¡Œ", "ç´¢å¼•", "æ–‡ä»¶æ ¼å¼"]
    },
    {
        "category": "RAGä¼˜åŒ–",
        "language": "zh",
        "question": "RAGç³»ç»Ÿä¸­çš„chunkingç­–ç•¥æœ‰å“ªäº›æœ€ä½³å®è·µï¼Ÿ",
        "expected_topics": ["chunk size", "overlap", "semantic", "metadata", "boundaries"]
    },
    {
        "category": "æ•…éšœæ’æŸ¥",
        "language": "zh",
        "question": "å¦‚æœMilvusè¿æ¥å¤±è´¥åº”è¯¥æ€ä¹ˆåŠï¼Ÿ",
        "expected_topics": ["Docker", "ç«¯å£", "é…ç½®", "é‡å¯"]
    },
]

def format_answer(answer: str, max_length: int = 600) -> str:
    """æ ¼å¼åŒ–ç­”æ¡ˆè¾“å‡ºï¼Œæ·»åŠ é€‚å½“çš„æ¢è¡Œå’Œæˆªæ–­"""
    lines = answer.split('\n')
    formatted_lines = []
    current_length = 0
    
    for line in lines:
        if current_length + len(line) > max_length:
            formatted_lines.append("   " + line[:max_length - current_length] + "...")
            formatted_lines.append("   [ç­”æ¡ˆå·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹è¯¦ç»†è¾“å‡º]")
            break
        formatted_lines.append("   " + line)
        current_length += len(line)
    
    return '\n'.join(formatted_lines)

def test_qa_demo():
    """è¿è¡ŒQ&Aæ¼”ç¤ºæµ‹è¯•"""
    logger.info("\n" + "="*120)
    logger.info("ğŸ¯ çŸ¥è¯†åº“Q&Aæ¼”ç¤ºæµ‹è¯•".center(120))
    logger.info("="*120)
    logger.info(f"\næ€»æµ‹è¯•é—®é¢˜æ•°: {len(TEST_QUESTIONS)}")
    logger.info("æµ‹è¯•å†…å®¹: ç³»ç»Ÿä½¿ç”¨ã€æŠ€æœ¯æ¶æ„ã€åŠŸèƒ½æŸ¥è¯¢ã€æŠ€æœ¯ç»†èŠ‚ã€å¤šè¯­è¨€ã€APIå¯¹æ¯”ã€çŸ¥è¯†åº“ç®¡ç†ã€RAGä¼˜åŒ–ã€æ•…éšœæ’æŸ¥")
    logger.info("="*120 + "\n")
    
    results = []
    total_time = 0
    
    for i, test_case in enumerate(TEST_QUESTIONS, 1):
        print("\n" + "="*120)
        print(f"ğŸ“ é—®é¢˜ {i}/{len(TEST_QUESTIONS)}")
        print("="*120)
        print(f"åˆ†ç±»: {test_case['category']}")
        print(f"è¯­è¨€: {test_case['language']}")
        print(f"é—®é¢˜: {test_case['question']}")
        print("-"*120)
        
        # æ‰§è¡ŒæŸ¥è¯¢
        start_time = time.time()
        try:
            result = agent.execute(test_case['question'])
            elapsed_time = time.time() - start_time
            total_time += elapsed_time
            
            answer = result.get('answer', 'æœªè·å–åˆ°ç­”æ¡ˆ')
            tools_used = result.get('tools_used', [])
            contexts_count = result.get('contexts_count', 0)
            has_context = result.get('has_context', False)
            tokens = result.get('tokens', {})
            workflow_type = result.get('workflow_type')
            workflow_engine = result.get('workflow_engine')
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            print(f"\nğŸ’¡ ç­”æ¡ˆ:")
            print(format_answer(answer))
            
            # æ˜¾ç¤ºå…ƒä¿¡æ¯
            print(f"\nğŸ“Š æ‰§è¡Œä¿¡æ¯:")
            print(f"   - å“åº”æ—¶é—´: {elapsed_time:.2f}ç§’")
            print(f"   - ä½¿ç”¨å·¥å…·: {', '.join(tools_used)}")
            print(f"   - æ£€ç´¢ä¸Šä¸‹æ–‡æ•°: {contexts_count}")
            print(f"   - ä½¿ç”¨çŸ¥è¯†åº“: {'æ˜¯' if has_context else 'å¦'}")
            
            if workflow_engine:
                print(f"   - å·¥ä½œæµå¼•æ“: {workflow_engine}")
                print(f"   - å·¥ä½œæµç±»å‹: {workflow_type}")
            
            if tokens:
                print(f"   - Tokenä½¿ç”¨: è¾“å…¥={tokens.get('input', 0)}, è¾“å‡º={tokens.get('output', 0)}, æ€»è®¡={tokens.get('total', 0)}")
            
            # æ£€æŸ¥é¢„æœŸä¸»é¢˜æ˜¯å¦å‡ºç°åœ¨ç­”æ¡ˆä¸­
            expected_topics = test_case.get('expected_topics', [])
            found_topics = [topic for topic in expected_topics if topic.lower() in answer.lower()]
            
            if found_topics:
                print(f"   - è¦†ç›–ä¸»é¢˜: {', '.join(found_topics)} ({len(found_topics)}/{len(expected_topics)})")
            
            # è¯„ä¼°ç­”æ¡ˆè´¨é‡
            quality_score = "ä¼˜ç§€" if len(found_topics) >= len(expected_topics) * 0.6 else "è‰¯å¥½" if len(found_topics) >= len(expected_topics) * 0.3 else "ä¸€èˆ¬"
            print(f"   - ç­”æ¡ˆè´¨é‡: {quality_score}")
            
            results.append({
                'question': test_case['question'],
                'category': test_case['category'],
                'success': True,
                'time': elapsed_time,
                'tools': tools_used,
                'contexts': contexts_count,
                'quality': quality_score,
                'topics_found': len(found_topics),
                'topics_total': len(expected_topics),
                'answer_length': len(answer),
            })
            
            print("="*120)
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            total_time += elapsed_time
            
            logger.error(f"âŒ é—®é¢˜æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            results.append({
                'question': test_case['question'],
                'category': test_case['category'],
                'success': False,
                'time': elapsed_time,
                'error': str(e)
            })
            
            print("="*120)
        
        # æ¯ä¸ªé—®é¢˜ä¹‹é—´ç¨ä½œåœé¡¿
        time.sleep(0.5)
    
    # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
    print("\n" + "="*120)
    print("ğŸ“Š æµ‹è¯•æ€»ç»“".center(120))
    print("="*120)
    
    successful = [r for r in results if r.get('success')]
    failed = [r for r in results if not r.get('success')]
    
    print(f"\nâœ… æˆåŠŸ: {len(successful)}/{len(TEST_QUESTIONS)}")
    print(f"âŒ å¤±è´¥: {len(failed)}/{len(TEST_QUESTIONS)}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {total_time/len(TEST_QUESTIONS):.2f}ç§’")
    
    if successful:
        avg_contexts = sum(r.get('contexts', 0) for r in successful) / len(successful)
        print(f"ğŸ“š å¹³å‡æ£€ç´¢ä¸Šä¸‹æ–‡æ•°: {avg_contexts:.1f}")
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        print(f"\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
        categories = {}
        for r in successful:
            cat = r['category']
            if cat not in categories:
                categories[cat] = {'count': 0, 'avg_time': 0, 'total_time': 0}
            categories[cat]['count'] += 1
            categories[cat]['total_time'] += r['time']
        
        for cat, stats in categories.items():
            stats['avg_time'] = stats['total_time'] / stats['count']
            print(f"   - {cat}: {stats['count']}ä¸ªé—®é¢˜, å¹³å‡{stats['avg_time']:.2f}ç§’")
        
        # è´¨é‡ç»Ÿè®¡
        print(f"\nâ­ è´¨é‡åˆ†å¸ƒ:")
        quality_counts = {}
        for r in successful:
            quality = r.get('quality', 'æœªçŸ¥')
            quality_counts[quality] = quality_counts.get(quality, 0) + 1
        
        for quality, count in quality_counts.items():
            print(f"   - {quality}: {count}ä¸ª ({count/len(successful)*100:.1f}%)")
        
        # å·¥å…·ä½¿ç”¨ç»Ÿè®¡
        print(f"\nğŸ› ï¸  å·¥å…·ä½¿ç”¨ç»Ÿè®¡:")
        tool_counts = {}
        for r in successful:
            for tool in r.get('tools', []):
                tool_counts[tool] = tool_counts.get(tool, 0) + 1
        
        for tool, count in sorted(tool_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {tool}: {count}æ¬¡")
    
    if failed:
        print(f"\nâŒ å¤±è´¥é—®é¢˜:")
        for r in failed:
            print(f"   - {r['question'][:60]}... (é”™è¯¯: {r.get('error', 'æœªçŸ¥')})")
    
    print("\n" + "="*120)
    print("âœ… Q&Aæ¼”ç¤ºæµ‹è¯•å®Œæˆï¼".center(120))
    print("="*120 + "\n")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°æ–‡ä»¶
    output_file = 'docs/QA_TEST_RESULTS.md'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# çŸ¥è¯†åº“Q&Aæµ‹è¯•ç»“æœ\n\n")
        f.write(f"æµ‹è¯•æ—¥æœŸ: 2025-11-17\n")
        f.write(f"æ€»é—®é¢˜æ•°: {len(TEST_QUESTIONS)}\n")
        f.write(f"æˆåŠŸ: {len(successful)}\n")
        f.write(f"å¤±è´¥: {len(failed)}\n\n")
        f.write("---\n\n")
        
        for i, (test_case, result) in enumerate(zip(TEST_QUESTIONS, results), 1):
            f.write(f"## é—®é¢˜ {i}: {test_case['category']}\n\n")
            f.write(f"**é—®é¢˜**: {test_case['question']}\n\n")
            
            if result.get('success'):
                # é‡æ–°è·å–å®Œæ•´ç­”æ¡ˆï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                f.write(f"**ç­”æ¡ˆ**: [è§æµ‹è¯•è¾“å‡º]\n\n")
                f.write(f"**å“åº”æ—¶é—´**: {result['time']:.2f}ç§’\n\n")
                f.write(f"**ä½¿ç”¨å·¥å…·**: {', '.join(result['tools'])}\n\n")
                f.write(f"**æ£€ç´¢ä¸Šä¸‹æ–‡**: {result['contexts']}ä¸ª\n\n")
                f.write(f"**ç­”æ¡ˆè´¨é‡**: {result['quality']}\n\n")
            else:
                f.write(f"**çŠ¶æ€**: âŒ å¤±è´¥\n\n")
                f.write(f"**é”™è¯¯**: {result.get('error')}\n\n")
            
            f.write("---\n\n")
    
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return results

if __name__ == "__main__":
    results = test_qa_demo()

