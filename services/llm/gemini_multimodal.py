#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Gemini å¤šæ¨¡æ€å®¢æˆ·ç«¯
æ”¯æŒæ–‡æœ¬+å›¾ç‰‡çš„æ··åˆè¾“å…¥ï¼Œé€‚ç”¨äºè§†è§‰é—®ç­”ã€å›¾ç‰‡åˆ†æç­‰åœºæ™¯
"""
import base64
import io
from typing import List, Dict, Optional, Union, Any
from PIL import Image
import google.generativeai as genai
from services.core import settings, logger


class GeminiMultimodalClient:
    """
    Gemini å¤šæ¨¡æ€å®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼š
    1. æ”¯æŒå¤šå¼ å›¾ç‰‡+æ–‡æœ¬è¾“å…¥
    2. å›¾ç‰‡æ ¼å¼è½¬æ¢å’Œä¼˜åŒ–
    3. OCRæ–‡å­—è¯†åˆ«
    4. å›¾ç‰‡å†…å®¹åˆ†æ
    """
    
    def __init__(self, api_key: Optional[str] = None, model_name: str = "gemini-2.0-flash-exp"):
        """
        åˆå§‹åŒ–Geminiå¤šæ¨¡æ€å®¢æˆ·ç«¯
        
        Args:
            api_key: Gemini APIå¯†é’¥
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨gemini-2.0-flash-expï¼ˆæ”¯æŒæœ€æ–°å¤šæ¨¡æ€åŠŸèƒ½ï¼‰
        """
        self.api_key = api_key or settings.GEMINI_API_KEY
        if not self.api_key:
            raise ValueError("Gemini APIå¯†é’¥æœªé…ç½®")
        
        genai.configure(api_key=self.api_key)
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        self.supported_formats = ['jpeg', 'jpg', 'png', 'gif', 'webp']
        
        logger.info(f"âœ… Geminiå¤šæ¨¡æ€å®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œæ¨¡å‹: {model_name}")
    
    def _decode_image(self, image_data: str) -> Image.Image:
        """
        è§£ç Base64å›¾ç‰‡
        
        Args:
            image_data: Base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
            
        Returns:
            PIL Imageå¯¹è±¡
        """
        try:
            # ç§»é™¤data URLå‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
            if ',' in image_data:
                image_data = image_data.split(',')[1]
            
            # è§£ç Base64
            image_bytes = base64.b64decode(image_data)
            image = Image.open(io.BytesIO(image_bytes))
            
            return image
        except Exception as e:
            logger.error(f"å›¾ç‰‡è§£ç å¤±è´¥: {e}")
            raise ValueError(f"æ— æ•ˆçš„å›¾ç‰‡æ•°æ®: {e}")
    
    def _optimize_image(self, image: Image.Image, max_size: int = 1024) -> Image.Image:
        """
        ä¼˜åŒ–å›¾ç‰‡å¤§å°ï¼ˆé™ä½APIæˆæœ¬ï¼‰
        
        Args:
            image: PIL Imageå¯¹è±¡
            max_size: æœ€å¤§è¾¹é•¿
            
        Returns:
            ä¼˜åŒ–åçš„å›¾ç‰‡
        """
        # å¦‚æœå›¾ç‰‡å·²ç»è¶³å¤Ÿå°ï¼Œç›´æ¥è¿”å›
        if max(image.size) <= max_size:
            return image
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        ratio = max_size / max(image.size)
        new_size = tuple(int(dim * ratio) for dim in image.size)
        
        # ç¼©æ”¾å›¾ç‰‡
        optimized = image.resize(new_size, Image.Resampling.LANCZOS)
        logger.info(f"å›¾ç‰‡å·²ä¼˜åŒ–: {image.size} -> {optimized.size}")
        
        return optimized
    
    def _prepare_image_for_gemini(self, image_data: str, optimize: bool = True) -> Image.Image:
        """
        å‡†å¤‡å›¾ç‰‡ä¾›Geminiä½¿ç”¨
        
        Args:
            image_data: Base64ç¼–ç çš„å›¾ç‰‡
            optimize: æ˜¯å¦ä¼˜åŒ–å›¾ç‰‡å¤§å°
            
        Returns:
            å‡†å¤‡å¥½çš„PIL Imageå¯¹è±¡
        """
        image = self._decode_image(image_data)
        
        # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆGeminiè¦æ±‚ï¼‰
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # ä¼˜åŒ–å›¾ç‰‡å¤§å°
        if optimize:
            image = self._optimize_image(image)
        
        return image
    
    def query_with_images(
        self,
        query: str,
        images: List[str],
        system_prompt: Optional[str] = None,
        max_tokens: int = 2048,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨å›¾ç‰‡+æ–‡æœ¬è¿›è¡ŒæŸ¥è¯¢
        
        Args:
            query: æ–‡æœ¬æŸ¥è¯¢
            images: Base64ç¼–ç çš„å›¾ç‰‡åˆ—è¡¨
            system_prompt: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            {
                "content": "ç”Ÿæˆçš„ç­”æ¡ˆ",
                "model": "æ¨¡å‹åç§°",
                "input_tokens": è¾“å…¥tokenæ•°,
                "output_tokens": è¾“å‡ºtokenæ•°,
                "total_tokens": æ€»tokenæ•°,
                "images_processed": å¤„ç†çš„å›¾ç‰‡æ•°é‡
            }
        """
        try:
            # å‡†å¤‡å›¾ç‰‡
            pil_images = []
            for i, img_data in enumerate(images):
                try:
                    pil_img = self._prepare_image_for_gemini(img_data)
                    pil_images.append(pil_img)
                    logger.info(f"âœ… å›¾ç‰‡ {i+1}/{len(images)} å·²å‡†å¤‡å°±ç»ª")
                except Exception as e:
                    logger.error(f"âŒ å›¾ç‰‡ {i+1} å¤„ç†å¤±è´¥: {e}")
                    # ç»§ç»­å¤„ç†å…¶ä»–å›¾ç‰‡
                    continue
            
            if not pil_images:
                return {"error": "æ‰€æœ‰å›¾ç‰‡å¤„ç†å¤±è´¥"}
            
            # æ„å»ºå®Œæ•´æç¤º
            if system_prompt:
                full_prompt = f"{system_prompt}\n\n{query}"
            else:
                full_prompt = query
            
            # æ„å»ºå†…å®¹åˆ—è¡¨ï¼š[å›¾ç‰‡1, å›¾ç‰‡2, ..., æ–‡æœ¬]
            content = pil_images + [full_prompt]
            
            # ç”Ÿæˆé…ç½®
            generation_config = genai.GenerationConfig(
                max_output_tokens=max_tokens,
                temperature=temperature
            )
            
            logger.info(f"ğŸ–¼ï¸ å¼€å§‹å¤šæ¨¡æ€æŸ¥è¯¢: {len(pil_images)}å¼ å›¾ç‰‡ + æ–‡æœ¬")
            
            # è°ƒç”¨Gemini
            response = self.model.generate_content(
                content,
                generation_config=generation_config
            )
            
            # æå–ç»“æœ
            answer = response.text
            
            # æå–tokenä½¿ç”¨ä¿¡æ¯
            usage = response.usage_metadata
            result = {
                "content": answer,
                "model": self.model_name,
                "input_tokens": usage.prompt_token_count,
                "output_tokens": usage.candidates_token_count,
                "total_tokens": usage.total_token_count,
                "images_processed": len(pil_images)
            }
            
            logger.info(f"âœ… å¤šæ¨¡æ€æŸ¥è¯¢æˆåŠŸï¼Œå¤„ç†{len(pil_images)}å¼ å›¾ç‰‡ï¼Œtokenä½¿ç”¨: {result['total_tokens']}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Geminiå¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {e}")
            return {"error": f"å¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {str(e)}"}
    
    def extract_text_from_image(self, image_data: str) -> Dict[str, Any]:
        """
        ä»å›¾ç‰‡ä¸­æå–æ–‡å­—ï¼ˆOCRï¼‰
        
        Args:
            image_data: Base64ç¼–ç çš„å›¾ç‰‡
            
        Returns:
            {
                "text": "è¯†åˆ«å‡ºçš„æ–‡æœ¬",
                "confidence": ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰,
                "model": "æ¨¡å‹åç§°"
            }
        """
        try:
            # å‡†å¤‡å›¾ç‰‡
            pil_image = self._prepare_image_for_gemini(image_data)
            
            # ä½¿ç”¨Geminiè¿›è¡ŒOCR
            prompt = """è¯·æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚
è¦æ±‚ï¼š
1. ä¿æŒåŸæœ‰çš„æ’ç‰ˆç»“æ„å’Œæ¢è¡Œ
2. å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¯·ç”¨Markdownæ ¼å¼è¡¨ç¤º
3. å¦‚æœæœ‰å¤šç§è¯­è¨€ï¼Œè¯·å…¨éƒ¨æå–
4. åªè¾“å‡ºæ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æè¿°

æå–çš„æ–‡å­—ï¼š"""
            
            content = [pil_image, prompt]
            response = self.model.generate_content(content)
            
            extracted_text = response.text.strip()
            
            # Geminiä¸æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨å›ºå®šå€¼
            confidence = 0.95 if len(extracted_text) > 0 else 0.0
            
            result = {
                "text": extracted_text,
                "confidence": confidence,
                "model": self.model_name,
                "language": "auto"  # Geminiè‡ªåŠ¨æ£€æµ‹è¯­è¨€
            }
            
            logger.info(f"âœ… OCRæˆåŠŸï¼Œæå–{len(extracted_text)}å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"âŒ OCRå¤±è´¥: {e}")
            return {
                "error": f"OCRå¤±è´¥: {str(e)}",
                "text": "",
                "confidence": 0.0
            }
    
    def analyze_image(self, image_data: str, analysis_type: str = "general") -> Dict[str, Any]:
        """
        åˆ†æå›¾ç‰‡å†…å®¹
        
        Args:
            image_data: Base64ç¼–ç çš„å›¾ç‰‡
            analysis_type: åˆ†æç±»å‹
                - general: é€šç”¨æè¿°
                - detailed: è¯¦ç»†åˆ†æ
                - objects: ç‰©ä½“è¯†åˆ«
                - scene: åœºæ™¯è¯†åˆ«
                - sentiment: æƒ…æ„Ÿåˆ†æ
                
        Returns:
            {
                "description": "å›¾ç‰‡æè¿°",
                "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"],
                "model": "æ¨¡å‹åç§°"
            }
        """
        try:
            pil_image = self._prepare_image_for_gemini(image_data)
            
            # æ ¹æ®åˆ†æç±»å‹æ„å»ºæç¤º
            prompts = {
                "general": "è¯·ç®€è¦æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚",
                "detailed": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ï¼š1)ä¸»è¦å¯¹è±¡ 2)åœºæ™¯ç¯å¢ƒ 3)é¢œè‰²å’Œå…‰çº¿ 4)æ•´ä½“æ°›å›´",
                "objects": "è¯·åˆ—å‡ºè¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰ç‰©ä½“ï¼Œç”¨é€—å·åˆ†éš”ã€‚",
                "scene": "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡çš„åœºæ™¯ç±»å‹ï¼ˆå¦‚ï¼šå®¤å†…/å®¤å¤–ã€è‡ªç„¶/åŸå¸‚ã€ç™½å¤©/å¤œæ™šç­‰ï¼‰",
                "sentiment": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¼ è¾¾çš„æƒ…æ„Ÿæˆ–æ°›å›´ã€‚"
            }
            
            prompt = prompts.get(analysis_type, prompts["general"])
            
            content = [pil_image, prompt]
            response = self.model.generate_content(content)
            
            description = response.text.strip()
            
            # ç®€å•æå–æ ‡ç­¾ï¼ˆä»æè¿°ä¸­ï¼‰
            tags = []
            if analysis_type == "objects":
                tags = [tag.strip() for tag in description.split(',')]
            
            result = {
                "description": description,
                "tags": tags,
                "analysis_type": analysis_type,
                "model": self.model_name
            }
            
            logger.info(f"âœ… å›¾ç‰‡åˆ†ææˆåŠŸï¼Œç±»å‹: {analysis_type}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å›¾ç‰‡åˆ†æå¤±è´¥: {e}")
            return {
                "error": f"å›¾ç‰‡åˆ†æå¤±è´¥: {str(e)}",
                "description": "",
                "tags": []
            }


# å…¨å±€å•ä¾‹
_multimodal_client = None

def get_multimodal_client() -> GeminiMultimodalClient:
    """è·å–å¤šæ¨¡æ€å®¢æˆ·ç«¯å•ä¾‹"""
    global _multimodal_client
    if _multimodal_client is None:
        _multimodal_client = GeminiMultimodalClient()
    return _multimodal_client

