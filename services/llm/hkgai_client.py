"""
LLMå®¢æˆ·ç«¯ - å°è£…HKGAIClient
"""
import requests
from typing import Dict, Optional
from services.core.config import settings
from services.core.logger import logger


class HKGAIClient:
    """HKGAIClientåŒ…è£…ç±»ï¼Œç”¨äºè°ƒç”¨LLM API"""
    
    def __init__(self):
        self.base_url = settings.HKGAI_BASE_URL
        self.api_key = settings.HKGAI_API_KEY
        self.model_id = settings.HKGAI_MODEL_ID
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def chat(self, system_prompt: str, user_prompt: str, 
             max_tokens: int = 500, temperature: float = 0.7) -> Dict:
        """
        å‘é€èŠå¤©è¯·æ±‚åˆ°LLM API
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            åŒ…å«contentå’Œrawæ•°æ®çš„å­—å…¸
        """
        endpoint = f"{self.base_url}/chat/completions"
        payload = {
            "model": self.model_id,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": max_tokens,
            "temperature": temperature
        }

        logger.info(f"ğŸ”µ è°ƒç”¨HKGAI API: {endpoint}")
        logger.debug(f"è¯·æ±‚Payload: model={self.model_id}, max_tokens={max_tokens}, temperature={temperature}")
        logger.debug(f"ç”¨æˆ·æç¤º: {user_prompt[:100]}...")
        
        try:
            response = requests.post(endpoint, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            logger.info(f"âœ… HKGAI APIè°ƒç”¨æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ HKGAI APIè°ƒç”¨å¤±è´¥: {e}")
            return {"error": str(e)}

        data = response.json()
        content = ""
        try:
            choices = data.get("choices", [])
            if choices:
                first = choices[0] if isinstance(choices[0], dict) else {}
                # Chat schema
                message = first.get("message") or {}
                content = (message.get("content") or "").strip()
                # Fallback to text-based schema
                if not content:
                    content = (first.get("text") or "").strip()
        except Exception:
            pass

        if not content:
            finish_reason = None
            try:
                finish_reason = data.get("choices", [{}])[0].get("finish_reason")
            except Exception:
                pass
            logger.warning(f"âš ï¸ HKGAIè¿”å›ç©ºå†…å®¹ï¼Œfinish_reason: {finish_reason}")
            return {
                "content": "",
                "warning": "Empty content returned. Possible causes: wrong endpoint for model, content filter, or max_tokens too small.",
                "finish_reason": finish_reason,
                "raw": data
            }

        logger.info(f"âœ… HKGAIè¿”å›å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        logger.debug(f"å†…å®¹é¢„è§ˆ: {content[:100]}...")
        return {"content": content, "raw": data}


# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
llm_client = HKGAIClient()

