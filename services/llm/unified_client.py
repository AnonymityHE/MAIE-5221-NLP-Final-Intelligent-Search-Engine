"""
ç»Ÿä¸€LLMå®¢æˆ·ç«¯ - æ”¯æŒHKGAIå’ŒGeminiï¼Œè‡ªåŠ¨fallbackæœºåˆ¶
"""
from typing import Dict, Optional
from services.core.config import settings
from services.llm.hkgai_client import HKGAIClient
from services.llm.gemini_client import GeminiClient
from services.core.logger import logger


class UnifiedLLMClient:
    """ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ï¼Œæ”¯æŒå¤šä¸ªAPIæä¾›å•†ï¼Œå¸¦è‡ªåŠ¨fallback"""
    
    def __init__(self):
        # åˆå§‹åŒ–HKGAIå®¢æˆ·ç«¯ï¼ˆå‘åå…¼å®¹ï¼‰
        self.hkgai_client = HKGAIClient()
        
        # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if settings.GEMINI_ENABLED and settings.GEMINI_API_KEY:
            self.gemini_client = GeminiClient(settings.GEMINI_API_KEY)
            logger.info("âœ… Geminiå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå¯ä½œä¸ºfallback")
        else:
            self.gemini_client = None
            logger.warning("âš ï¸  Geminiå®¢æˆ·ç«¯æœªé…ç½®ï¼Œæ— fallbacké€‰é¡¹")
        
        # è®°å½•HKGAIå¤±è´¥æ¬¡æ•°ï¼ˆç”¨äºæ™ºèƒ½fallbackï¼‰
        self.hkgai_failure_count = 0
    
    def chat(self, system_prompt: str, user_prompt: str,
             max_tokens: int = 2048,
             temperature: float = 0.7,
             model: Optional[str] = None,
             provider: str = "hkgai") -> Dict:
        """
        å‘é€èŠå¤©è¯·æ±‚ï¼ˆå¸¦è‡ªåŠ¨fallbackï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            model: æ¨¡å‹åç§°ï¼ˆä»…Geminiä½¿ç”¨ï¼‰
            provider: APIæä¾›å•† ("gemini" æˆ– "hkgai")
            
        Returns:
            åŒ…å«contentã€tokenä½¿ç”¨é‡ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å¦‚æœæ˜ç¡®æŒ‡å®šä½¿ç”¨Gemini
        if provider.lower() == "gemini" and self.gemini_client:
            return self._call_gemini(system_prompt, user_prompt, model, max_tokens, temperature)
        
        # å¦‚æœHKGAIè¿ç»­å¤±è´¥å¤šæ¬¡ï¼Œç›´æ¥ä½¿ç”¨Gemini
        if self.hkgai_failure_count >= 3 and self.gemini_client:
            logger.warning(f"âš ï¸  HKGAIå·²è¿ç»­å¤±è´¥{self.hkgai_failure_count}æ¬¡ï¼Œç›´æ¥ä½¿ç”¨Gemini")
            return self._call_gemini(system_prompt, user_prompt, model, max_tokens, temperature)
        
        # å°è¯•ä½¿ç”¨HKGAI
        try:
            result = self.hkgai_client.chat(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
            if "error" in result:
                self.hkgai_failure_count += 1
                logger.warning(f"âš ï¸  HKGAIè°ƒç”¨å¤±è´¥ (å¤±è´¥è®¡æ•°: {self.hkgai_failure_count})")
                
                # è‡ªåŠ¨fallbackåˆ°Gemini
                if self.gemini_client:
                    logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°Gemini API")
                    return self._call_gemini(system_prompt, user_prompt, model, max_tokens, temperature)
                else:
                    logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„fallback API")
                    return result
            else:
                # æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                if self.hkgai_failure_count > 0:
                    logger.info(f"âœ… HKGAIæ¢å¤æ­£å¸¸ï¼Œé‡ç½®å¤±è´¥è®¡æ•°ï¼ˆä¹‹å‰: {self.hkgai_failure_count}ï¼‰")
                    self.hkgai_failure_count = 0
                result["provider"] = "hkgai"
                return result
                
        except Exception as e:
            self.hkgai_failure_count += 1
            logger.error(f"âŒ HKGAIå¼‚å¸¸: {e} (å¤±è´¥è®¡æ•°: {self.hkgai_failure_count})")
            
            # è‡ªåŠ¨fallbackåˆ°Gemini
            if self.gemini_client:
                logger.info("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°Gemini API")
                return self._call_gemini(system_prompt, user_prompt, model, max_tokens, temperature)
            else:
                return {"error": str(e), "provider": "hkgai"}
    
    def _call_gemini(self, system_prompt: str, user_prompt: str, 
                     model: Optional[str], max_tokens: int, temperature: float) -> Dict:
        """è°ƒç”¨Gemini API"""
        try:
            result = self.gemini_client.chat(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                model=model or settings.GEMINI_DEFAULT_MODEL,
                max_tokens=max_tokens,
                temperature=temperature
            )
            result["provider"] = "gemini"
            return result
        except Exception as e:
            logger.error(f"âŒ Gemini APIè°ƒç”¨ä¹Ÿå¤±è´¥: {e}")
            return {"error": str(e), "provider": "gemini"}
    
    def get_supported_models(self) -> Dict:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        result = {
            "default_provider": "hkgai",
            "providers": ["hkgai"],
            "hkgai_models": [settings.HKGAI_MODEL_ID],
            "hkgai_info": {
                "model": settings.HKGAI_MODEL_ID,
                "description": "é»˜è®¤LLMæä¾›å•†ï¼Œç¨³å®šå¯é "
            }
        }
        
        if self.gemini_client:
            gemini_info = self.gemini_client.get_supported_models()
            result["providers"].append("gemini")
            result["gemini_models"] = gemini_info.get("supported_models", [])
            result["gemini_info"] = {
                "default_model": gemini_info.get("default_model"),
                "models": gemini_info.get("supported_models", []),
                "description": "å¤‡é€‰LLMæä¾›å•†ï¼Œæ”¯æŒå¤šä¸ªGeminiæ¨¡å‹ï¼Œå¸¦ç”¨é‡ç›‘æ§"
            }
        
        return result


# å…¨å±€ç»Ÿä¸€å®¢æˆ·ç«¯å®ä¾‹
unified_llm_client = UnifiedLLMClient()

