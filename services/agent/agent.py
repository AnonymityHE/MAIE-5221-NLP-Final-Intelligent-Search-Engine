"""
æ™ºèƒ½Agent - æ ¹æ®é—®é¢˜ç±»åž‹è‡ªåŠ¨é€‰æ‹©å’Œä½¿ç”¨åˆé€‚çš„å·¥å…·
æ”¯æŒï¼šæœ¬åœ°RAGã€ç½‘é¡µæœç´¢ã€å¤©æ°”ã€é‡‘èžã€äº¤é€šæŸ¥è¯¢
æ–°å¢žï¼šåŠ¨æ€å·¥ä½œæµæ”¯æŒï¼ˆå¤šæ­¥éª¤æŸ¥è¯¢ï¼‰
- LLMé©±åŠ¨çš„æ™ºèƒ½å·¥ä½œæµè§„åˆ’ï¼ˆä¼˜å…ˆï¼‰
- åŸºäºŽè§„åˆ™çš„å·¥ä½œæµæ¨¡æ¿ï¼ˆFallbackï¼‰
"""
from typing import Dict, List, Optional
from services.llm.unified_client import unified_llm_client
from services.agent.tools.local_rag_tool import get_local_knowledge_context
from services.agent.tools.web_search_tool import get_web_search_context
from services.agent.tools.weather_tool import get_weather_context
from services.agent.tools.finance_tool import get_finance_context
from services.agent.tools.transport_tool import get_transport_context
from services.agent.workflow import WorkflowEngine, get_workflow_engine
from services.core.logger import logger
import re

# å°è¯•å¯¼å…¥LangGraphç‰ˆæœ¬ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
try:
    from services.agent.workflow_langgraph import (
        LangGraphWorkflowEngine, 
        get_langgraph_workflow_engine, 
        LANGGRAPH_AVAILABLE as LG_AVAILABLE
    )
    LANGGRAPH_AVAILABLE = LG_AVAILABLE
    get_langgraph_workflow_engine = get_langgraph_workflow_engine
except ImportError:
    LANGGRAPH_AVAILABLE = False
    LangGraphWorkflowEngine = None
    get_langgraph_workflow_engine = None

# å¯¼å…¥LLMé©±åŠ¨çš„å·¥ä½œæµæ¨¡å—
try:
    from services.agent.workflow_llm_planner import get_llm_workflow_planner
    from services.agent.workflow_dynamic import get_dynamic_workflow_engine
    LLM_WORKFLOW_AVAILABLE = True
except ImportError:
    logger.warning("LLMå·¥ä½œæµæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºäºŽè§„åˆ™çš„å·¥ä½œæµ")
    LLM_WORKFLOW_AVAILABLE = False
    get_llm_workflow_planner = None
    get_dynamic_workflow_engine = None


class RAGAgent:
    """RAG Agent - æ ¹æ®é—®é¢˜ç±»åž‹æ™ºèƒ½é€‰æ‹©å·¥å…·ï¼ˆæ”¯æŒå¤šç§å·¥å…·ï¼‰"""
    
    def __init__(self):
        self.tools = {
            "local_rag": get_local_knowledge_context,
            "web_search": get_web_search_context,
            "weather": get_weather_context,
            "finance": get_finance_context,
            "transport": get_transport_context
        }
        
        # åˆå§‹åŒ–LLMé©±åŠ¨çš„å·¥ä½œæµç³»ç»Ÿï¼ˆä¼˜å…ˆï¼‰
        self.llm_planner = None
        self.dynamic_engine = None
        if LLM_WORKFLOW_AVAILABLE:
            try:
                tool_names = list(self.tools.keys())
                self.llm_planner = get_llm_workflow_planner(tool_names)
                self.dynamic_engine = get_dynamic_workflow_engine(self.tools)
                logger.info("âœ¨ ä½¿ç”¨LLMé©±åŠ¨çš„æ™ºèƒ½å·¥ä½œæµç³»ç»Ÿ")
            except Exception as e:
                logger.warning(f"LLMå·¥ä½œæµç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.llm_planner = None
                self.dynamic_engine = None
        
        # åˆå§‹åŒ–åŸºäºŽè§„åˆ™çš„å·¥ä½œæµå¼•æ“Žï¼ˆä½œä¸ºfallbackï¼‰
        if LANGGRAPH_AVAILABLE:
            try:
                self.workflow_engine = get_langgraph_workflow_engine(self.tools)
                logger.info("ðŸ“‹ LangGraphå·¥ä½œæµå¼•æ“Žå·²å°±ç»ªï¼ˆä½œä¸ºfallbackï¼‰")
            except Exception as e:
                logger.warning(f"LangGraphå·¥ä½œæµå¼•æ“Žåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨è‡ªå®šä¹‰å¼•æ“Ž: {e}")
                self.workflow_engine = get_workflow_engine(self.tools)
        else:
            self.workflow_engine = get_workflow_engine(self.tools)
            logger.info("ðŸ“‹ è‡ªå®šä¹‰å·¥ä½œæµå¼•æ“Žå·²å°±ç»ªï¼ˆä½œä¸ºfallbackï¼‰")
    
    def detect_question_type(self, query: str) -> List[str]:
        """
        æ£€æµ‹é—®é¢˜ç±»åž‹ï¼Œè¿”å›žåº”è¯¥ä½¿ç”¨çš„å·¥å…·åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æŽ’åºï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            å·¥å…·åç§°åˆ—è¡¨ï¼ˆå¦‚æžœè¿”å›žç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºç›´æŽ¥è°ƒç”¨LLMä¸ä½¿ç”¨ä»»ä½•å·¥å…·ï¼‰
        """
        query_lower = query.lower()
        tools = []
        
        # ðŸŒ ç¿»è¯‘/è¯­è¨€å­¦ä¹ é—®é¢˜ - ç›´æŽ¥ç”¨LLMï¼Œä¸éœ€è¦ä»»ä½•å·¥å…·
        translation_keywords = [
            "æ€Žä¹ˆè¯´", "æ€Žä¹ˆè¯»", "å‘éŸ³", "ç¿»è¯‘", "ç”¨ç²¤è¯­", "ç”¨æ™®é€šè¯", "ç”¨è‹±æ–‡",
            "how to say", "how do you say", "pronounce", "pronunciation", 
            "translation", "translate", "in cantonese", "in english", "in chinese"
        ]
        if any(keyword in query_lower for keyword in translation_keywords):
            logger.info("ðŸŒ æ£€æµ‹åˆ°ç¿»è¯‘/è¯­è¨€å­¦ä¹ é—®é¢˜ï¼Œç›´æŽ¥è°ƒç”¨LLMï¼ˆä¸ä½¿ç”¨RAGï¼‰")
            return []  # ç©ºåˆ—è¡¨è¡¨ç¤ºä¸ä½¿ç”¨ä»»ä½•å·¥å…·
        
        # æ£€æµ‹åŽ†å²æ—¶é—´å…³é”®è¯ï¼ˆæ˜¨å¤©ã€ä¸Šå‘¨ã€ä¸Šæœˆç­‰ï¼‰
        # åŽ†å²æŸ¥è¯¢é€šå¸¸éœ€è¦web_searchï¼Œå› ä¸ºå®žæ—¶å·¥å…·å¯èƒ½ä¸æ”¯æŒåŽ†å²æ•°æ®
        historical_keywords = ["yesterday", "æ˜¨å¤©", "last week", "ä¸Šå‘¨", "last month", "ä¸Šæœˆ", "past", "è¿‡åŽ»", "ä»¥å‰", "ä¹‹å‰"]
        is_historical_query = any(kw in query_lower for kw in historical_keywords)
        
        # æ£€æµ‹é‡‘èžæŸ¥è¯¢
        if any(kw in query_lower for kw in ["stock", "è‚¡ç¥¨", "price", "è‚¡ä»·", "crypto", "åŠ å¯†è´§å¸", "bitcoin", "btc", "ethereum", "eth"]):
            tools.append("finance")
        
        # æ£€æµ‹äº¤é€šæŸ¥è¯¢
        if any(kw in query_lower for kw in ["travel", "æ—…è¡Œ", "route", "è·¯çº¿", "time", "æ—¶é—´", "how long", "å¤šä¹…", "distance", "è·ç¦»"]):
            tools.append("transport")
        
        # æ£€æµ‹å¤©æ°”æŸ¥è¯¢
        # æ³¨æ„ï¼šå¦‚æžœæ˜¯åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œåº”è¯¥ä½¿ç”¨web_searchè€Œä¸æ˜¯weatherå·¥å…·
        if any(kw in query_lower for kw in ["weather", "å¤©æ°”", "rain", "ä¸‹é›¨", "temperature", "æ¸©åº¦", "forecast", "é¢„æŠ¥", "cloud", "äº‘", "æ€Žéº¼æ¨£", "æ€Žä¹ˆæ ·"]):
            if is_historical_query:
                # åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼šä½¿ç”¨web_searchï¼ˆweatherå·¥å…·åªæ”¯æŒå½“å‰å¤©æ°”ï¼‰
                tools.append("web_search")
                logger.info("æ£€æµ‹åˆ°åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œä½¿ç”¨web_searchå·¥å…·")
            else:
                # å½“å‰å¤©æ°”æŸ¥è¯¢ï¼šä½¿ç”¨weatherå·¥å…·
                tools.append("weather")
        
        # æ£€æµ‹å®žæ—¶/æ–°é—»æŸ¥è¯¢ï¼ˆéœ€è¦ç½‘é¡µæœç´¢ï¼‰
        # æ³¨æ„ï¼šå¦‚æžœå·²ç»æœ‰weather/finance/transportå·¥å…·ä¸”ä¸æ˜¯åŽ†å²æŸ¥è¯¢ï¼Œä¸è¦æ·»åŠ web_search
        if not tools and any(kw in query_lower for kw in ["latest", "æœ€æ–°", "news", "æ–°é—»", "current", "çŽ°åœ¨", "today", "ä»Šå¤©", "recent", "æœ€è¿‘", "recently"]):
            tools.append("web_search")
        
        # é»˜è®¤ä½¿ç”¨æœ¬åœ°RAGï¼ˆå¦‚æžœè¿˜æ²¡æœ‰å·¥å…·ï¼‰
        if not tools:
            tools.append("local_rag")
        else:
            # å¦‚æžœå·²ç»æœ‰å…¶ä»–å·¥å…·ï¼Œå°†local_ragä½œä¸ºå¤‡é€‰
            tools.append("local_rag")
        
        return tools
    
    def extract_location(self, query: str) -> Optional[str]:
        """ä»ŽæŸ¥è¯¢ä¸­æå–åœ°ç‚¹ä¿¡æ¯"""
        query_lower = query.lower()
        
        # å¸¸è§åœ°ç‚¹æ˜ å°„
        common_locations = {
            "hong kong": "Hong Kong",
            "é¦™æ¸¯": "Hong Kong",
            "beijing": "Beijing",
            "åŒ—äº¬": "Beijing",
            "shanghai": "Shanghai",
            "ä¸Šæµ·": "Shanghai",
            "taipei": "Taipei",
            "å°åŒ—": "Taipei",
            "tokyo": "Tokyo",
            "ä¸œäº¬": "Tokyo",
            "new york": "New York",
            "london": "London",
            "london": "London"
        }
        
        for key, value in common_locations.items():
            if key in query_lower:
                return value
        
        return None
    
    def execute(self, query: str, model: Optional[str] = None) -> Dict:
        """
        æ‰§è¡ŒAgentæŽ¨ç†ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·å¹¶èŽ·å–ç­”æ¡ˆ
        æ”¯æŒåŠ¨æ€å·¥ä½œæµï¼ˆå¤šæ­¥éª¤æŸ¥è¯¢ï¼‰
        
        å·¥ä½œæµæ‰§è¡Œä¼˜å…ˆçº§ï¼š
        1. LLMé©±åŠ¨çš„æ™ºèƒ½å·¥ä½œæµï¼ˆä¼˜å…ˆï¼‰
        2. åŸºäºŽè§„åˆ™çš„å·¥ä½œæµæ¨¡æ¿ï¼ˆfallbackï¼‰
        3. å•å·¥å…·ç›´æŽ¥è°ƒç”¨ï¼ˆç®€å•æŸ¥è¯¢ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            model: å¯é€‰çš„æ¨¡åž‹åç§°
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€ä½¿ç”¨çš„å·¥å…·å’Œä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        # 0. å°è¯•LLMé©±åŠ¨çš„å·¥ä½œæµè§„åˆ’ï¼ˆä¼˜å…ˆï¼‰
        if self.llm_planner and self.dynamic_engine:
            try:
                logger.info("ðŸ§  å°è¯•LLMé©±åŠ¨çš„å·¥ä½œæµè§„åˆ’...")
                plan = self.llm_planner.analyze_query(query)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥ä½œæµä¸”ç½®ä¿¡åº¦è¶³å¤Ÿ
                if plan.requires_workflow and plan.confidence >= 0.4:
                    logger.info(f"âœ… LLMè§„åˆ’æˆåŠŸ (ç½®ä¿¡åº¦: {plan.confidence:.2f}), ä½¿ç”¨åŠ¨æ€å·¥ä½œæµ")
                    return self._execute_llm_workflow(query, model, plan)
                else:
                    logger.info(f"â„¹ï¸  LLMè®¤ä¸ºä¸éœ€è¦å·¥ä½œæµ (ç½®ä¿¡åº¦: {plan.confidence:.2f}), å°è¯•è§„åˆ™å¼•æ“Ž")
            except Exception as e:
                logger.warning(f"âš ï¸  LLMå·¥ä½œæµè§„åˆ’å¤±è´¥: {e}, å›žé€€åˆ°è§„åˆ™å¼•æ“Ž")
        
        # 1. å›žé€€åˆ°åŸºäºŽè§„åˆ™çš„å·¥ä½œæµæ£€æµ‹
        workflow_type = self.workflow_engine.detect_workflow_type(query)
        if workflow_type:
            logger.info(f"ðŸ“‹ è§„åˆ™å¼•æ“Žæ£€æµ‹åˆ°å·¥ä½œæµ: {workflow_type}")
            return self._execute_rule_based_workflow(query, model, workflow_type)
        
        # 1. æ£€æµ‹é—®é¢˜ç±»åž‹ï¼Œå†³å®šä½¿ç”¨å“ªäº›å·¥å…·ï¼ˆåŽŸæœ‰é€»è¾‘ï¼‰
        tools_to_use = self.detect_question_type(query)
        
        # å¦‚æžœè¿”å›žç©ºåˆ—è¡¨ï¼Œè¡¨ç¤ºç›´æŽ¥è°ƒç”¨LLMï¼ˆå¦‚ç¿»è¯‘é—®é¢˜ï¼‰
        if not tools_to_use:
            logger.info("âš¡ ç›´æŽ¥è°ƒç”¨LLMï¼Œä¸ä½¿ç”¨ä»»ä½•å·¥å…·")
            llm_result = unified_llm_client.chat(
                system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿è¯­è¨€ç¿»è¯‘å’Œæ•™å­¦ã€‚è¯·ç›´æŽ¥ã€ç®€æ´åœ°å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚",
                user_prompt=query,
                max_tokens=2048,
                temperature=0.7,
                model=model,
                provider="hkgai"
            )
            
            answer = llm_result.get("content", "æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            if "error" in llm_result:
                logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_result['error']}")
                answer = f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›žç­”è¿™ä¸ªé—®é¢˜ã€‚"
            
            tokens_info = None
            if "input_tokens" in llm_result:
                tokens_info = {
                    "input": llm_result.get("input_tokens", 0),
                    "output": llm_result.get("output_tokens", 0),
                    "total": llm_result.get("total_tokens", 0)
                }
            
            return {
                "answer": answer,
                "tools_used": ["direct_llm"],
                "contexts_count": 0,
                "has_context": False,
                "tokens": tokens_info,
                "model": llm_result.get("model")
            }
        
        # 2. æŒ‰ä¼˜å…ˆçº§æ”¶é›†ä¸Šä¸‹æ–‡
        contexts = []
        tools_used = []
        
        # å¯¹äºŽç‰¹å®šç±»åž‹çš„é—®é¢˜ï¼Œåªä½¿ç”¨å¯¹åº”çš„å·¥å…·ï¼ˆä¸fallbackï¼‰
        query_lower = query.lower()
        
        # æ£€æµ‹åŽ†å²æ—¶é—´å…³é”®è¯
        historical_keywords = ["yesterday", "æ˜¨å¤©", "last week", "ä¸Šå‘¨", "last month", "ä¸Šæœˆ", "past", "è¿‡åŽ»", "ä»¥å‰", "ä¹‹å‰"]
        is_historical_query = any(kw in query_lower for kw in historical_keywords)
        
        is_weather_query = any(kw in query_lower for kw in ["weather", "å¤©æ°”", "rain", "ä¸‹é›¨", "temperature", "æ¸©åº¦", "forecast", "é¢„æŠ¥", "æ€Žéº¼æ¨£", "æ€Žä¹ˆæ ·"])
        is_finance_query = any(kw in query_lower for kw in ["stock", "è‚¡ç¥¨", "price", "è‚¡ä»·", "crypto", "åŠ å¯†è´§å¸", "bitcoin", "btc"])
        is_transport_query = any(kw in query_lower for kw in ["travel", "æ—…è¡Œ", "route", "è·¯çº¿", "time", "æ—¶é—´", "how long", "å¤šä¹…"])
        
        # å®žæ—¶æŸ¥è¯¢ï¼šåªæœ‰åœ¨æ²¡æœ‰ç‰¹å®šå·¥å…·ï¼ˆweather/finance/transportï¼‰æ—¶æ‰ä½¿ç”¨web_search
        # ä½†æ˜¯åŽ†å²å¤©æ°”æŸ¥è¯¢åº”è¯¥ä½¿ç”¨web_searchï¼Œæ‰€ä»¥éœ€è¦ç‰¹æ®Šå¤„ç†
        is_realtime_query = not (is_weather_query or is_finance_query or is_transport_query) and any(kw in query_lower for kw in ["latest", "æœ€æ–°", "news", "æ–°é—»", "current", "çŽ°åœ¨", "today", "ä»Šå¤©", "recent", "æœ€è¿‘"])
        
        # å¦‚æžœå¤©æ°”æŸ¥è¯¢æ˜¯åŽ†å²æŸ¥è¯¢ï¼Œåº”è¯¥ä½¿ç”¨web_searchè€Œä¸æ˜¯weatherå·¥å…·
        if is_weather_query and is_historical_query:
            logger.info("æ£€æµ‹åˆ°åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œä¼˜å…ˆä½¿ç”¨web_searchå·¥å…·")
        
        for tool_name in tools_to_use:
            context = ""
            
            if tool_name == "finance":
                context = self.tools["finance"](query, num_results=3)
                if context:
                    contexts.append(f"[é‡‘èžä¿¡æ¯]\n{context}")
                    tools_used.append("finance")
                    logger.info("ä½¿ç”¨é‡‘èžå·¥å…·èŽ·å–ä¿¡æ¯")
                    if is_finance_query:
                        break
            
            elif tool_name == "transport":
                context = self.tools["transport"](query, num_results=3)
                if context:
                    contexts.append(f"[äº¤é€šä¿¡æ¯]\n{context}")
                    tools_used.append("transport")
                    logger.info("ä½¿ç”¨äº¤é€šå·¥å…·èŽ·å–ä¿¡æ¯")
                    if is_transport_query:
                        break
            
            elif tool_name == "weather":
                # å¦‚æžœæ˜¯åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œè·³è¿‡weatherå·¥å…·ï¼Œåº”è¯¥ä½¿ç”¨web_search
                if is_historical_query:
                    logger.info("åŽ†å²å¤©æ°”æŸ¥è¯¢è·³è¿‡weatherå·¥å…·ï¼Œå°†ä½¿ç”¨web_search")
                    continue
                
                location = self.extract_location(query) or "Hong Kong"
                context = self.tools["weather"](location)
                if context:
                    contexts.append(f"[å¤©æ°”ä¿¡æ¯]\n{context}")
                    tools_used.append("weather")
                    logger.info(f"ä½¿ç”¨å¤©æ°”å·¥å…·èŽ·å– {location} çš„å¤©æ°”ä¿¡æ¯")
                    # å¤©æ°”æŸ¥è¯¢æ˜¯ç¡®å®šçš„ï¼Œæ‰¾åˆ°å°±åœæ­¢
                    if is_weather_query:
                        break
                else:
                    # å¦‚æžœweatherå·¥å…·å¤±è´¥ï¼Œå¯¹äºŽåŽ†å²æŸ¥è¯¢åº”è¯¥fallbackåˆ°web_search
                    if is_historical_query:
                        logger.info("weatherå·¥å…·å¤±è´¥ï¼ŒåŽ†å²å¤©æ°”æŸ¥è¯¢fallbackåˆ°web_search")
                        # ä¸breakï¼Œç»§ç»­å°è¯•web_search
                        continue
            
            elif tool_name == "web_search":
                context = self.tools["web_search"](query, num_results=3)
                if context:
                    contexts.append(f"[ç½‘ç»œæœç´¢ç»“æžœ]\n{context}")
                    tools_used.append("web_search")
                    logger.info("ä½¿ç”¨ç½‘é¡µæœç´¢å·¥å…·èŽ·å–ä¿¡æ¯")
                    # å¯¹äºŽå®žæ—¶ä¿¡æ¯æŸ¥è¯¢æˆ–åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œå¦‚æžœç½‘é¡µæœç´¢æœ‰ç»“æžœå°±ä½¿ç”¨
                    if is_realtime_query or (is_weather_query and is_historical_query):
                        break
                else:
                    # å³ä½¿æ²¡æœ‰æœç´¢ç»“æžœï¼Œä¹Ÿæ ‡è®°å°è¯•äº†web_search
                    # å¯¹äºŽå®žæ—¶æŸ¥è¯¢æˆ–åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œå¦‚æžœæ²¡æœ‰æœç´¢åˆ°ç»“æžœï¼Œåº”è¯¥ç›´æŽ¥ä½¿ç”¨LLMå›žç­”
                    if is_realtime_query or (is_weather_query and is_historical_query):
                        tools_used.append("web_search_attempted")
                        logger.info("ç½‘é¡µæœç´¢æ— ç»“æžœï¼Œä½†å¯¹å®žæ—¶æŸ¥è¯¢/åŽ†å²å¤©æ°”æŸ¥è¯¢å°†ä½¿ç”¨LLMå›žç­”")
                        # å¯¹äºŽå®žæ—¶æŸ¥è¯¢æˆ–åŽ†å²å¤©æ°”æŸ¥è¯¢ï¼Œå³ä½¿æœç´¢æ— ç»“æžœï¼Œä¹Ÿç›´æŽ¥ä½¿ç”¨LLMï¼ˆä¸å°è¯•local_ragï¼‰
                        break
            
            elif tool_name == "local_rag":
                context = self.tools["local_rag"](query)
                if context:
                    contexts.append(f"[æœ¬åœ°çŸ¥è¯†åº“]\n{context}")
                    tools_used.append("local_rag")
                    logger.info("ä½¿ç”¨æœ¬åœ°RAGå·¥å…·èŽ·å–ä¿¡æ¯")
                    # å¦‚æžœæ˜¯ç¡®å®šçš„çŸ¥è¯†åº“æŸ¥è¯¢ï¼Œæœ‰ç»“æžœå°±ä½¿ç”¨
                    if not is_weather_query and not is_realtime_query and not is_finance_query and not is_transport_query:
                        break
            
            # å¦‚æžœå·²ç»æ”¶é›†åˆ°ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œåœæ­¢æœç´¢
            if contexts and (is_weather_query or is_finance_query or is_transport_query or (is_realtime_query and "web_search" in tools_used)):
                break
        
        # 3. æž„å»ºPromptå¹¶è°ƒç”¨LLM
        if contexts:
            # æœ‰å·¥å…·ç»“æžœï¼Œä½¿ç”¨å¢žå¼ºå›žç­”
            all_context = "\n\n".join(contexts)
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚è¯·åŸºäºŽæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›žç­”é—®é¢˜ã€‚"
                "å¦‚æžœä¸Šä¸‹æ–‡ä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚"
            )
            user_prompt = f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{all_context}\n\né—®é¢˜ï¼š{query}\n\nè¯·åŸºäºŽä¸Šä¸‹æ–‡å›žç­”ä¸Šè¿°é—®é¢˜ã€‚"
        elif "web_search_attempted" in tools_used:
            # å°è¯•äº†ç½‘é¡µæœç´¢ä½†æ²¡æœ‰ç»“æžœï¼Œå¯¹äºŽå®žæ—¶æŸ¥è¯¢æˆ–åŽ†å²å¤©æ°”æŸ¥è¯¢ç›´æŽ¥ç”¨LLMå›žç­”
            if is_weather_query and is_historical_query:
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®çš„æ˜¯åŽ†å²å¤©æ°”ä¿¡æ¯ã€‚"
                    "è™½ç„¶ç½‘é¡µæœç´¢æ²¡æœ‰è¿”å›žç»“æžœï¼Œä½†è¯·åŸºäºŽä½ çš„çŸ¥è¯†å°½å¯èƒ½å›žç­”é—®é¢˜ã€‚"
                    "å¦‚æžœæ— æ³•æä¾›å‡†ç¡®çš„åŽ†å²å¤©æ°”æ•°æ®ï¼Œè¯·è¯šå®žè¯´æ˜Žã€‚"
                )
            else:
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®çš„æ˜¯å®žæ—¶ä¿¡æ¯æˆ–æœ€æ–°æ–°é—»ã€‚"
                    "è™½ç„¶ç½‘é¡µæœç´¢æ²¡æœ‰è¿”å›žç»“æžœï¼Œä½†è¯·åŸºäºŽä½ çš„çŸ¥è¯†å°½å¯èƒ½å›žç­”é—®é¢˜ã€‚"
                )
            user_prompt = query
            tools_used = ["web_search_attempted", "direct_llm"]
        else:
            # æ²¡æœ‰å·¥å…·ç»“æžœï¼Œç›´æŽ¥å›žç­”
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æŽ¥å›žç­”é—®é¢˜ã€‚"
            user_prompt = query
            if not tools_used:
                tools_used = ["direct_llm"]
        
        # 4. è°ƒç”¨LLMï¼ˆä½¿ç”¨ç»Ÿä¸€å®¢æˆ·ç«¯ï¼Œé»˜è®¤ä½¿ç”¨HKGAIï¼‰
        logger.info(f"ðŸ¤– å‡†å¤‡è°ƒç”¨LLMï¼ˆHKGAIï¼‰ï¼ŒæŸ¥è¯¢: '{query[:50]}...'")
        llm_result = unified_llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=2048,
            temperature=0.7,
            model=model,  # å¦‚æžœæŒ‡å®šäº†Geminiæ¨¡åž‹ï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨Gemini
            provider="hkgai"  # Agenté»˜è®¤ä½¿ç”¨HKGAI
        )
        
        if "error" in llm_result:
            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: {llm_result['error']}")
            answer = f"LLMè°ƒç”¨å¤±è´¥: {llm_result['error']}"
        else:
            answer = llm_result.get("content", "æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            logger.info(f"âœ… LLMè¿”å›žç­”æ¡ˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            logger.debug(f"ç­”æ¡ˆé¢„è§ˆ: {answer[:100]}...")
        
        # æå–tokenä½¿ç”¨ä¿¡æ¯
        tokens_info = None
        if "input_tokens" in llm_result:
            tokens_info = {
                "input": llm_result.get("input_tokens", 0),
                "output": llm_result.get("output_tokens", 0),
                "total": llm_result.get("total_tokens", 0)
            }
        
        return {
            "answer": answer,
            "tools_used": tools_used,
            "contexts_count": len(contexts),
            "has_context": len(contexts) > 0,
            "tokens": tokens_info,
            "model": llm_result.get("model")
        }
    
    def _execute_llm_workflow(self, query: str, model: Optional[str], plan) -> Dict:
        """
        æ‰§è¡ŒLLMé©±åŠ¨çš„åŠ¨æ€å·¥ä½œæµ
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            model: å¯é€‰çš„æ¨¡åž‹åç§°
            plan: LLMç”Ÿæˆçš„å·¥ä½œæµè®¡åˆ’
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€ä½¿ç”¨çš„å·¥å…·å’Œä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        logger.info(f"ðŸš€ å¼€å§‹æ‰§è¡ŒLLMé©±åŠ¨çš„å·¥ä½œæµ: {plan.workflow_type}")
        
        # 1. ä½¿ç”¨åŠ¨æ€æ‰§è¡Œå¼•æ“Žæ‰§è¡Œè®¡åˆ’
        execution_context = self.dynamic_engine.execute(plan, query)
        
        # 2. ç»¼åˆæ‰§è¡Œç»“æžœ
        workflow_context = self.dynamic_engine.synthesize_results(execution_context)
        
        # 3. èŽ·å–å·¥å…·ä½¿ç”¨æ‘˜è¦
        tools_used = self.dynamic_engine.get_tool_usage_summary(execution_context)
        
        # 4. æž„å»ºPromptå¹¶è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        if workflow_context:
            # æ£€æµ‹æ˜¯å¦æ˜¯ç¿»è¯‘/è¯­è¨€å­¦ä¹ ç±»é—®é¢˜
            is_translation_query = any(keyword in query for keyword in [
                "æ€Žä¹ˆè¯´", "æ€Žä¹ˆè¯»", "å‘éŸ³", "ç¿»è¯‘", "ç”¨ç²¤è¯­", "ç”¨æ™®é€šè¯", "ç”¨è‹±æ–‡",
                "how to say", "pronounce", "translation", "in Cantonese", "in English"
            ])
            
            if is_translation_query:
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­è¨€å­¦ä¹ åŠ©æ‰‹ã€‚ç”¨æˆ·æƒ³çŸ¥é“æŸä¸ªè¯æˆ–å¥å­åœ¨å¦ä¸€ç§è¯­è¨€ä¸­æ€Žä¹ˆè¯´ã€‚\n"
                    "è¯·ç›´æŽ¥ç»™å‡ºç›®æ ‡è¯­è¨€çš„è¯´æ³•ï¼Œä¸è¦ä»‹ç»ç³»ç»ŸåŠŸèƒ½æˆ–åç¦»ä¸»é¢˜ã€‚\n\n"
                    "**å›žç­”æ ¼å¼**ï¼š\n"
                    "1. å…ˆç›´æŽ¥ç»™å‡ºç›®æ ‡è¯­è¨€çš„è¯´æ³•ï¼ˆå¦‚ï¼šç²¤è¯­ï¼šå””è¯¥å””å¥½é è¿‘è½¦é—¨ï¼‰\n"
                    "2. å¦‚æžœçŸ¥é“ï¼Œå¯ä»¥ç®€å•è¡¥å……å‘éŸ³æˆ–ç”¨æ³•è¯´æ˜Ž\n\n"
                    "**æ³¨æ„**ï¼šå¦‚æžœæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸Žç¿»è¯‘æ— å…³ï¼ˆå¦‚ç³»ç»ŸåŠŸèƒ½ä»‹ç»ï¼‰ï¼Œè¯·å¿½ç•¥å®ƒä»¬ï¼Œä¸“æ³¨äºŽå›žç­”ç¿»è¯‘é—®é¢˜ã€‚"
                )
            else:
                system_prompt = (
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚æˆ‘å·²ç»é€šè¿‡æ™ºèƒ½å·¥ä½œæµç³»ç»Ÿæ‰§è¡Œäº†å¤šä¸ªæ­¥éª¤æ¥æ”¶é›†ä¿¡æ¯ã€‚"
                    "è¯·åŸºäºŽä»¥ä¸‹å·¥ä½œæµæ‰§è¡Œç»“æžœï¼Œç»¼åˆåˆ†æžå¹¶å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
                    "æ³¨æ„ï¼šç»“æžœå¯èƒ½æ¥è‡ªä¸åŒçš„æ•°æ®æºï¼ˆç½‘é¡µæœç´¢ã€é‡‘èžAPIã€å¤©æ°”APIç­‰ï¼‰ï¼Œè¯·æ•´åˆè¿™äº›ä¿¡æ¯ç»™å‡ºå…¨é¢çš„ç­”æ¡ˆã€‚"
                )
            
            user_prompt = f"åŽŸå§‹é—®é¢˜ï¼š{query}\n\n{workflow_context}\n\nè¯·åŸºäºŽä»¥ä¸Šä¿¡æ¯ç»¼åˆå›žç­”åŽŸå§‹é—®é¢˜ã€‚"
            logger.info("ä½¿ç”¨LLMå·¥ä½œæµç»“æžœæž„å»ºPrompt")
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œå›žé€€åˆ°æ™®é€šLLMå›žç­”
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æŽ¥å›žç­”é—®é¢˜ã€‚"
            user_prompt = query
            tools_used = ["llm_workflow_failed"]
            logger.warning("LLMå·¥ä½œæµæ‰§è¡Œæ— ç»“æžœï¼Œå›žé€€åˆ°ç›´æŽ¥å›žç­”")
        
        # 5. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        llm_result = unified_llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=2048,
            temperature=0.7,
            model=model,
            provider="hkgai"
        )
        
        answer = llm_result.get("content", "æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
        if "error" in llm_result:
            logger.error(f"LLMå·¥ä½œæµæ¨¡å¼ä¸‹LLMè°ƒç”¨å¤±è´¥: {llm_result['error']}")
            answer = f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œä½†LLMç”Ÿæˆç­”æ¡ˆå¤±è´¥: {llm_result['error']}"
        
        # æå–tokenä½¿ç”¨ä¿¡æ¯
        tokens_info = None
        if "input_tokens" in llm_result:
            tokens_info = {
                "input": llm_result.get("input_tokens", 0),
                "output": llm_result.get("output_tokens", 0),
                "total": llm_result.get("total_tokens", 0)
            }
        
        return {
            "answer": answer,
            "tools_used": tools_used,
            "contexts_count": len(execution_context.completed_steps),
            "has_context": len(workflow_context) > 0,
            "tokens": tokens_info,
            "model": llm_result.get("model"),
            "workflow_type": plan.workflow_type,
            "workflow_engine": "llm_driven",
            "workflow_confidence": plan.confidence,
            "workflow_steps_completed": len(execution_context.completed_steps)
        }
    
    def _execute_rule_based_workflow(self, query: str, model: Optional[str], workflow_type: str) -> Dict:
        """
        æ‰§è¡ŒåŸºäºŽè§„åˆ™çš„å·¥ä½œæµï¼ˆåŽŸæœ‰é€»è¾‘ï¼Œä½œä¸ºfallbackï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            model: å¯é€‰çš„æ¨¡åž‹åç§°
            workflow_type: å·¥ä½œæµç±»åž‹
            
        Returns:
            åŒ…å«ç­”æ¡ˆã€ä½¿ç”¨çš„å·¥å…·å’Œä¸Šä¸‹æ–‡çš„å­—å…¸
        """
        # 1. æ£€æŸ¥å·¥ä½œæµå¼•æ“Žç±»åž‹å¹¶æ‰§è¡Œ
        if isinstance(self.workflow_engine, LangGraphWorkflowEngine):
            # ä½¿ç”¨LangGraphå·¥ä½œæµ
            workflow_state = self.workflow_engine.execute_workflow(query, workflow_type)
            workflow_context = self.workflow_engine.synthesize_workflow_results(workflow_state)
            tools_used = [f"workflow:{step}" for step in workflow_state.get("steps_completed", [])]
            steps_completed = len(workflow_state.get("steps_completed", []))
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰å·¥ä½œæµ
            workflow_steps = self.workflow_engine.build_workflow(query, workflow_type)
            
            if not workflow_steps:
                logger.warning("å·¥ä½œæµæž„å»ºå¤±è´¥ï¼Œå›žé€€åˆ°æ™®é€šå·¥å…·è°ƒç”¨")
                return self._execute_normal(query, model)
            
            # æ‰§è¡Œå·¥ä½œæµ
            workflow_state = self.workflow_engine.execute_workflow(query, workflow_steps)
            
            # ç»¼åˆå·¥ä½œæµç»“æžœ
            workflow_context = self.workflow_engine.synthesize_workflow_results(workflow_state)
            
            # æž„å»ºå·¥å…·ä½¿ç”¨åˆ—è¡¨
            tools_used = [f"workflow:{step.name}" for step in workflow_state.steps if step.status.value == "completed"]
            steps_completed = sum(1 for s in workflow_state.steps if s.status.value == "completed")
        
        # 4. æž„å»ºPromptå¹¶è°ƒç”¨LLMç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        
        if workflow_context:
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·æå‡ºäº†ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ï¼Œæˆ‘å·²ç»é€šè¿‡å¤šä¸ªæ­¥éª¤æ”¶é›†äº†ç›¸å…³ä¿¡æ¯ã€‚"
                "è¯·åŸºäºŽä»¥ä¸‹å·¥ä½œæµæ‰§è¡Œç»“æžœï¼Œç»¼åˆåˆ†æžå¹¶å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
            )
            user_prompt = f"åŽŸå§‹é—®é¢˜ï¼š{query}\n\nå·¥ä½œæµæ‰§è¡Œç»“æžœï¼š\n\n{workflow_context}\n\nè¯·åŸºäºŽä»¥ä¸Šä¿¡æ¯ç»¼åˆå›žç­”åŽŸå§‹é—®é¢˜ã€‚"
            logger.info("ä½¿ç”¨å·¥ä½œæµç»“æžœæž„å»ºPrompt")
        else:
            # å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼Œå›žé€€åˆ°æ™®é€šLLMå›žç­”
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æŽ¥å›žç­”é—®é¢˜ã€‚"
            user_prompt = query
            tools_used = ["workflow_failed"]
        
        # 5. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        llm_result = unified_llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=2048,
            temperature=0.7,
            model=model,
            provider="hkgai"
        )
        
        answer = llm_result.get("content", "æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
        if "error" in llm_result:
            logger.error(f"å·¥ä½œæµæ¨¡å¼ä¸‹LLMè°ƒç”¨å¤±è´¥: {llm_result['error']}")
            answer = f"å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œä½†LLMç”Ÿæˆç­”æ¡ˆå¤±è´¥: {llm_result['error']}"
        
        # æå–tokenä½¿ç”¨ä¿¡æ¯
        tokens_info = None
        if "input_tokens" in llm_result:
            tokens_info = {
                "input": llm_result.get("input_tokens", 0),
                "output": llm_result.get("output_tokens", 0),
                "total": llm_result.get("total_tokens", 0)
            }
        
        return {
            "answer": answer,
            "tools_used": tools_used,
            "contexts_count": steps_completed if isinstance(self.workflow_engine, LangGraphWorkflowEngine) else len(workflow_state.steps),
            "has_context": len(workflow_context) > 0,
            "tokens": tokens_info,
            "model": llm_result.get("model"),
            "workflow_type": workflow_type,
            "workflow_engine": "rule_based",  # æ ‡è®°ä¸ºåŸºäºŽè§„åˆ™çš„å·¥ä½œæµ
            "workflow_steps_completed": steps_completed
        }
    
    def _execute_normal(self, query: str, model: Optional[str]) -> Dict:
        """æ™®é€šæ‰§è¡Œï¼ˆåŽŸæœ‰çš„executeé€»è¾‘ï¼Œç”¨äºŽéžå·¥ä½œæµæŸ¥è¯¢ï¼‰"""
        # è¿™ä¸ªæ–¹æ³•æ˜¯ä¸ºäº†é¿å…å¾ªçŽ¯è°ƒç”¨ï¼Œä½†å®žé™…ä¸Šåº”è¯¥ä½¿ç”¨executeæ–¹æ³•
        # è¿™é‡Œç›´æŽ¥è°ƒç”¨executeçš„é€»è¾‘ï¼ˆä½†ä¸æ£€æµ‹å·¥ä½œæµï¼‰
        tools_to_use = self.detect_question_type(query)
        
        # æŒ‰ä¼˜å…ˆçº§æ”¶é›†ä¸Šä¸‹æ–‡
        contexts = []
        tools_used = []
        
        query_lower = query.lower()
        is_weather_query = any(kw in query_lower for kw in ["weather", "å¤©æ°”", "rain", "ä¸‹é›¨", "temperature", "æ¸©åº¦", "forecast", "é¢„æŠ¥"])
        is_finance_query = any(kw in query_lower for kw in ["stock", "è‚¡ç¥¨", "price", "è‚¡ä»·", "crypto", "åŠ å¯†è´§å¸", "bitcoin", "btc"])
        is_transport_query = any(kw in query_lower for kw in ["travel", "æ—…è¡Œ", "route", "è·¯çº¿", "time", "æ—¶é—´", "how long", "å¤šä¹…"])
        is_realtime_query = any(kw in query_lower for kw in ["latest", "æœ€æ–°", "news", "æ–°é—»", "current", "çŽ°åœ¨", "today", "ä»Šå¤©", "recent", "æœ€è¿‘"])
        
        for tool_name in tools_to_use:
            context = ""
            
            if tool_name == "finance":
                context = self.tools["finance"](query, num_results=3)
                if context:
                    contexts.append(f"[é‡‘èžä¿¡æ¯]\n{context}")
                    tools_used.append("finance")
                    if is_finance_query:
                        break
            
            elif tool_name == "transport":
                context = self.tools["transport"](query, num_results=3)
                if context:
                    contexts.append(f"[äº¤é€šä¿¡æ¯]\n{context}")
                    tools_used.append("transport")
                    if is_transport_query:
                        break
            
            elif tool_name == "weather":
                location = self.extract_location(query) or "Hong Kong"
                context = self.tools["weather"](location)
                if context:
                    contexts.append(f"[å¤©æ°”ä¿¡æ¯]\n{context}")
                    tools_used.append("weather")
                    if is_weather_query:
                        break
            
            elif tool_name == "web_search":
                context = self.tools["web_search"](query, num_results=3)
                if context:
                    contexts.append(f"[ç½‘ç»œæœç´¢ç»“æžœ]\n{context}")
                    tools_used.append("web_search")
                    if is_realtime_query:
                        break
                else:
                    if is_realtime_query:
                        tools_used.append("web_search_attempted")
                        break
            
            elif tool_name == "local_rag":
                context = self.tools["local_rag"](query)
                if context:
                    contexts.append(f"[æœ¬åœ°çŸ¥è¯†åº“]\n{context}")
                    tools_used.append("local_rag")
                    if not is_weather_query and not is_realtime_query and not is_finance_query and not is_transport_query:
                        break
            
            if contexts and (is_weather_query or is_finance_query or is_transport_query or (is_realtime_query and "web_search" in tools_used)):
                break
        
        # æž„å»ºPromptå¹¶è°ƒç”¨LLM
        if contexts:
            all_context = "\n\n".join(contexts)
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½AIåŠ©æ‰‹ã€‚è¯·åŸºäºŽæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›žç­”é—®é¢˜ã€‚"
                "å¦‚æžœä¸Šä¸‹æ–‡ä¸­åŒ…å«ç›¸å…³ä¿¡æ¯ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨è¿™äº›ä¿¡æ¯ã€‚"
            )
            user_prompt = f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š\n\n{all_context}\n\né—®é¢˜ï¼š{query}\n\nè¯·åŸºäºŽä¸Šä¸‹æ–‡å›žç­”ä¸Šè¿°é—®é¢˜ã€‚"
        elif "web_search_attempted" in tools_used:
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®çš„æ˜¯å®žæ—¶ä¿¡æ¯æˆ–æœ€æ–°æ–°é—»ã€‚"
                "è™½ç„¶ç½‘é¡µæœç´¢æ²¡æœ‰è¿”å›žç»“æžœï¼Œä½†è¯·åŸºäºŽä½ çš„çŸ¥è¯†å°½å¯èƒ½å›žç­”é—®é¢˜ã€‚"
            )
            user_prompt = query
            tools_used = ["web_search_attempted", "direct_llm"]
        else:
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ç›´æŽ¥å›žç­”é—®é¢˜ã€‚"
            user_prompt = query
            if not tools_used:
                tools_used = ["direct_llm"]
        
        # è°ƒç”¨LLM
        llm_result = unified_llm_client.chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            max_tokens=2048,
            temperature=0.7,
            model=model,
            provider="hkgai"
        )
        
        answer = llm_result.get("content", "æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
        
        tokens_info = None
        if "input_tokens" in llm_result:
            tokens_info = {
                "input": llm_result.get("input_tokens", 0),
                "output": llm_result.get("output_tokens", 0),
                "total": llm_result.get("total_tokens", 0)
            }
        
        return {
            "answer": answer,
            "tools_used": tools_used,
            "contexts_count": len(contexts),
            "has_context": len(contexts) > 0,
            "tokens": tokens_info,
            "model": llm_result.get("model")
        }


# å…¨å±€Agentå®žä¾‹
agent = RAGAgent()
